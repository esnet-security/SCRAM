## Design Decisions
An ever-growing document that holds information about choices we have made, so we can remember why we ended up designing
things this way.

#### nginx
The [cookiecutter](https://github.com/cookiecutter/cookiecutter-django) this repo is based off of, came with `traefik` 
as the reverse proxy used for `django`. We decided to move to nginx because
1. That would handle serving static files for us
1. It follows suit with how ESnet generally runs webservers

We did decide to run nginx inside a container instead of on the host because we would like to avoid dependency on the
underlying OS as much as possible.

#### Authentication
We are using OIDC as our authentication protocol of choice for production. The idea is that a group or groups are given
admin level access automatically based on information in their OIDC claims. With this in place, there should almost 
never be a time to use the admin account. We suggest not even storing the admin password that was autogenerated for you.
Instead, you should just use the `make pass-reset` target to set the password to a new (autogenerated elsewhere) 
password. This really should only happen if OIDC is currently broken, and you need to fix something from the webUI. For
instance, OIDC may be broken because you accidentally blocked the OIDC servers.

#### Expiration
Expiration is actually handled by a `docker-compose` healthcheck on the `django` container. We knew we'd need some sort
of job to run at a given interval. We initially thought of something like `celery`, but that seemed way too heavyweight
for our needs. `Cron` came to mind as well, but it seemed like we'd have to use the host OS to run the cronjob, and
we were trying to avoid caring about the underlying host as much as possible. The healthcheck allows us to run inside
docker, as well verify/autofix our django container if something goes wrong.

#### Database and State
We quickly identified a need for a highly available and shared postgres server to share data between instances in the 
same group (ie wan-scram, dc-scram, netlab-scram). Initially, we had planned on using Redis Streams to handle 
communication between instances in the same group, however, we discovered that redis does not handle IP addresses in any
form except for a string, which would not work for our purposes. We also investigaged running our own postgres cluster
via docker, using posgresql's new pub/sub features, and sharing the blocks via API. All of these options failed to 
meet our needs, so we have chosen to use [CloudSQL](https://cloud.google.com/sql) as a highly available, shared postgres
database.

We also liked this method as it allowed us to keep our state in the hosted postgres that is highly avaiable, easy to get
to from anywhere in our network, and backed up regularly. With state being so well protected, it means we can treat each
SCRAM instance as ephemeral. In fact, our Ansible deployment actually fully blows away a running instance and replaces 
it with a new one.

#### Websockets/Channels/Redis
While redis did not pan out for data sharing, it does serve well to back 
[`django channels` layers](https://channels.readthedocs.io/en/latest/topics/channel_layers.html). We use channel layers
to set up our "instance groups" of SCRAMs. Websockets is the method in which `translator` talks to `django` to get
information about what actions should be taken on which entries. This was chosen because this application inherently 
uses ongoing updates coming in at ad-hoc, unknown times to communicate bidirectionally between the `translator` and 
`django`.

#### API Client Connectivity
Any API clients will connect to the SCRAM REST API, which is over HTTPS. We can rely on this encryption to protect our
information being sent to the API from the client. We wanted to centralize the administration of the clients, so it 
seemed natural to use the built-in django admin site. The client is allowed to start up and hit a POST only endpoint on
first connection where it registers itself with SCRAM. We only allow the client to tell us its fqdn and a unique 
identifier. The SCRAM administrator is expected to go to the admin site to toggle the boolean for setting the client as 
authorized, as well as choose from any of the list of available actiontypes. During an entry's creation, we verify this
client is allowed to create an entry using the actiontype it is providing.

This model does allow for anyone to "register" a client, but until someone with admin level permissions goes to the admin
site and accepts the registration and sets authorized actiontypes, this client cannot create any entries and therefore,
not affect any sort of change. This endpoint is POST only, so nobody should be allowed to see what clients exist unless
they can access the admin site. Our main security concern after all this is a DoS by constantly POSTing to this endpoint,
which can be handled the same way any other DoS would be dealth with (ie likely blocked via SCRAM).

#### Configurable Payloads
Configurable payloads allows you to override certain data sent to the translator. Currently, this is ASN and BGP community.
In order to do so, you update the JSON dictionary inside the Web Socket Message entry in the admin page. One thing to note
is that you must include a "route" key in that dictionary with a string value of any kind. If you decide to change the 
key in the JSON payload whose value will contain the route being acted on field, you must change the route key in the 
dictionary to match that field.

#### Syncing
If you want two or more instances of SCRAM to share data between themselves we have a few ways of making sure that happens.

1. By depending on postgres, we can use a shared postgres instance to make sure both SCRAM instances have the same data
2. When a translator connects, it asks its local Django instance for all routes it already knows about in the DB and 
   announces those.
3. For normal syncing where both translators have been connected, we are currently using process_updates (since it runs 
   regularly) to grab new data out of the database that comes from other connected instances and reannounces those locally.

##### The Unblocking ProblemÂ® (PRs #157 and #193)

We had a bug where entries that had expired or been deleted on one SCRAM instance were never being unblocked on other instances. The original sync logic only looked at the `when` field (creation time), so it would happily find new entries but completely miss any that were modified, expired, or de-activated after creation. This was bad because things could get stuck being blocked forever (until gobgp and translator are restarted) and not show up in the web UI (because the database says it's not blocked.) Eventually, we should add "ghost" routes to the UI to show someone if there is something that's advertised by its translator but not in the database. We could even use a cute little ghost icon for it!

The fix takes advantage of the already existing `django-simple-history` models that track any entry modifications. To sync, we query the history models to see if something has changed in any way, and we just go ahead and re-send that to the translator, ensuring eventual consistency (since translator is idempotent).

We initially tried excluding any entries from the local instance from reprocessing (`originating_scram_instance != settings.SCRAM_HOSTNAME`) because we thought local entries would already be in the right state (i.e. if something blocked the entry on one host, it would then deactivate it on the same host), but this was wrong for expiration. When entries expire, it could be performed from any instance at any time (via the health check), not just the instance that it was originally blocked on, so the originating instance needs to reprocess even its own expired entries to guarantee that we send the `translator_remove` message. We removed the exclusion filter entirely (PR #193), so now all instances reprocess everything that changed. It's a bit redundant and wastes more resources, but it's idempotent and good enough until we re-design syncing from the ground up.

##### Future Work

Honestly, the use of compose health checks is kind of gross and we realize this. The `process_updates` polling approach works, but it's not elegant. We're probably looking at Celery or some other task runner to handle this properly. it'd be good to have something that can react to database changes on a message bus rather than polling every 30 seconds. But this gets things fixed for now, and the history-based seems solid enough for our needs.

#### Entries Page

We intentionally chose to only list the active entries. Our thinking is that the home page shows the most recent additions.
Then, if you went to the entries page, it would be overwhelmingly huge to show all the historical entries including the
ones that timed out/were deactivated. If you wanted to know about a specific entry even if it were not currently active
(to see the history of it say), you would likely be using the search anyway.
